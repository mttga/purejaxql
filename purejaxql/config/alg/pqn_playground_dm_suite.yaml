ALG_NAME: "pqn"

TOTAL_TIMESTEPS: 5e7
NUM_ENVS: 1024
NUM_STEPS: 32
NUM_EPOCHS: 4
NUM_MINIBATCHES: 32
ACTOR_UPDATE_FREQ: 1

ACTOR_HIDDEN_SIZES: [512, 256, 128]
CRITIC_HIDDEN_SIZES: [1024, 512, 256]
NUM_CRITICS: 2
NORM_TYPE: "layer_norm"
ACTIVATION: "relu"
NORM_INPUT: False #  normalize the input with batch norm
NORMALIZE_OBS: True # normalize the observation with running mean and std
NORMALIZE_REWARD: False # normalize the reward with running std of returns
USE_QLAMBDA: True

ANNEAL_LR: True
LR_START: 0.0001
LR_END: 0.0000005
LR_DECAY: 1.0
GAMMA: 0.995
LAMBDA: 0.70

# exploration
NOISE_DECAY: 1.0 # percentage in terms of total updates
NOISE_START: 1.0
NOISE_FINISH: 0.15

# regularization
THRESHOLD: 0.15
PENALTY_COEFF: 10.0  # e.g., 1.
MAX_GRAD_NORM: 1.0

# env specific
ENV_NAME: "CartpoleBalance"
ENV_KWARGS: {}

# evaluation
TEST_DURING_TRAINING: True 
TEST_INTERVAL: 0.1 # in terms of total updates, 0.1 will test 10 times during training
TEST_NUM_ENVS:  # if null, will be set to NUM_ENVS - can be set to a smaller number for faster eval, but might crash on some envs
