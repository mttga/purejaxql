ALG_NAME: "pqn_rnn"
TOTAL_TIMESTEPS: 1e9
TOTAL_TIMESTEPS_DECAY: 1e9 # will be used for decay functions, in case you want to test for less timesteps and keep decays same
NUM_ENVS: 1024 # parallel environments
MEMORY_WINDOW: 0 # steps of previous episode added in the rnn training horizon
NUM_STEPS: 128 # steps per environment in each update
EPS_START: 1.
EPS_FINISH: 0.005
EPS_DECAY: 0.1 # ratio of total updates
NUM_MINIBATCHES: 4 # minibatches per epoch
NUM_EPOCHS: 4 # minibatches per epoch
NORM_INPUT: True
NORM_TYPE: "batch_norm" # layer_norm or batch_norm
HIDDEN_SIZE: 512
NUM_LAYERS: 1
NUM_RNN_LAYERS: 1
ADD_LAST_ACTION: True # adds last action to the input of the rnn
LR: 0.0003
MAX_GRAD_NORM: 0.5
LR_LINEAR_DECAY: True
REW_SCALE: 1.
GAMMA: 0.99
LAMBDA: 0.5

# env specific
ENV_NAME: "Craftax-Symbolic-v1"
USE_OPTIMISTIC_RESETS: True
OPTIMISTIC_RESET_RATIO: 16
LOG_ACHIEVEMENTS: False

# evaluation
TEST_DURING_TRAINING: True 
TEST_INTERVAL: 0.01 # in terms of total updates
TEST_NUM_ENVS: 512
TEST_NUM_STEPS: 10000
EPS_TEST: 0.00 # 0 for greedy policy
